{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60035685",
   "metadata": {},
   "source": [
    "## LLM Supervised finetuning: LoRA and Full-parameter\n",
    "\n",
    "### 0. Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650b5ba0-24c0-4d42-a1f6-8a67257ffe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes datasets scipy ipywidgets accelerate loralib transformers peft fastchat--ignore-installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1026ef07-80d7-48cd-ab25-1e57fffa6bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 17 11:55:03 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   31C    P8             14W /  240W |       0MiB /   8192MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe390ed",
   "metadata": {},
   "source": [
    "#### checking GPU setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316afbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d71ee6-e998-4dfb-ae7e-fecd5251b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, copy, os, torch, transformers, logging\n",
    "from peft import LoraConfig, PeftModel\n",
    "from typing import Dict, Sequence, List\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# os.environ['HF_HOME'] = 'YOUR_LOCAL_PATH' # change here so that the cache is saved in your local path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778122f1-0892-4bb4-a81b-fdd1d53869fe",
   "metadata": {},
   "source": [
    "### 1. Load Base model \n",
    "\n",
    "Depends on the rescoures and training type, load the base model with or without quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab6c52d1-515a-4780-a234-678a94dcb77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Qwen/Qwen2.5-0.5B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a012df8-2321-4649-8759-d5050eb61d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\y'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\y'\n",
      "C:\\Users\\LI Jiangfan\\AppData\\Local\\Temp\\ipykernel_28948\\420996351.py:2: SyntaxWarning: invalid escape sequence '\\y'\n",
      "  local = 'E:\\yurui\\models'\n"
     ]
    }
   ],
   "source": [
    "# change the local path \n",
    "local = 'E:\\yurui\\models'\n",
    "\n",
    "# Here for Qlora, we use the BitsAndBytesConfig to specify the quantization configuration.\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, config=bnb_config,cache_dir=local,device_map='auto')\n",
    "\n",
    "\n",
    "# For Lora and full parameter tuning, we use the full-bit version of the model.\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map='auto',cache_dir=local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e944a0f-7565-40ba-ac3e-0c70dfd30ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,cache_dir=local)\n",
    "\n",
    "# Set the pad token to the end of the sequence\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bffa866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_messages_chatML(messages: List[Dict[str, str]],is_inference = False) -> str:\n",
    "    \"\"\"\n",
    "    Convert a list of messages to a chatML string.\n",
    "    chatML format:\n",
    "    <|im_start|> System: <|im_end|>\n",
    "    <|im_start|> User: Hello <|im_end|>\n",
    "    <|im_start|> Assistant: Hi! <|im_end|>\n",
    "    \"\"\"\n",
    "    chatML = \"\"\n",
    "    for message in messages:\n",
    "        chatML += f\"<|im_start|> {message['role']}: {message['content']} <|im_end|>\\n\"\n",
    "    \n",
    "    if is_inference:\n",
    "        chatML += f\"<|im_start|> Assistant:\\n\"\n",
    "    return chatML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eceac79b-1be0-4291-bdef-1de84e004fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   2657,     25,   2585,    525,    498,     30,    220, 151645,\n",
      "            198, 151644,  21388,    510,     40,   2776,   3730,   1632,     11,\n",
      "           9339,    369,  10161,      0,   2585,    911,    498,     30, 151645]],\n",
      "       device='cuda:0')\n",
      " User: How are you? \n",
      " Assistant:\n",
      "I'm doing well, thanks for asking! How about you?\n"
     ]
    }
   ],
   "source": [
    "# test if the model is working\n",
    "\n",
    "device = \"cuda\"\n",
    "def generate(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(input_ids, max_new_tokens=1024)\n",
    "    # print(outputs)\n",
    "    return tokenizer.decode(*outputs, skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "messages = [\n",
    "    {\"role\": \"User\", \"content\": \"How are you?\"}\n",
    "]\n",
    "\n",
    "chatml_messages = convert_messages_chatML(messages,is_inference=True)\n",
    "\n",
    "print(generate(chatml_messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d7952-f08d-447f-94fc-1bd8e6fbcdf7",
   "metadata": {},
   "source": [
    "### 2. Load dataset and tokenization\n",
    "Before this step, you need to first prepared you training dataset, with seperate instructions and answers\n",
    "\n",
    "In supervised finetuing, we need to set the label of instructions part to -100 (IGNORE_INDEX, might be different for different LLM), so that the model only learns to predict the answer based on the provided instrctions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a20606",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data format:\n",
    "a list of dictionaries, each dictionary contains an instruction and a response.\n",
    "the messages inside instruction and response follows the openai message format\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "example_data = [\n",
    "    {'instruction': [{'role': 'User', 'content': 'How are you?'}],\n",
    "        'response': [{'role': 'Assistant', 'content': 'I am fine, thank you!'}]},\n",
    "    {'instruction': [{'role': 'User', 'content': 'What is your name?'}],\n",
    "     'response': [{'role': 'Assistant', 'content': 'My name is Qwen.'}]},\n",
    "    {'instruction': [{'role': 'User', 'content': 'What is your favorite color?'}],\n",
    "     'response': [{'role': 'Assistant', 'content': 'I like blue.'}]},\n",
    "    {'instruction': [{'role': 'User', 'content': 'What is the weather today?'}],\n",
    "     'response': [{'role': 'Assistant', 'content': 'It is sunny today.'}]},\n",
    "]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f8166c-c960-4770-bad2-3eacd6b35d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "\n",
    "\n",
    "class InsrtuctionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        model_name: str,\n",
    "        data: List[Dict[str, List[Dict[str, str]]]]= None,\n",
    "        data_path : str = None,\n",
    "        ingore_instruction = True # whether to ingore the instruction\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        # load json\n",
    "        if not data and data_path.endswith('.jsonl'):\n",
    "            self.data = []\n",
    "            with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    self.data.append(json.loads(line))\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ingore_instruction = ingore_instruction\n",
    "        print(\"Data loaded\")\n",
    "        print(\"Current model: \", self.model_name)\n",
    "        print('currently, we are ingoring the instruction in the labels: ',self.ingore_instruction)\n",
    "        self.data = self.preprocess_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n",
    "        return self.data[index]\n",
    "    \n",
    "    def convert_messages_chatML(self, messages: List[Dict[str, str]],is_inference = False) -> str:\n",
    "        \"\"\"\n",
    "        Convert a list of messages to a chatML string.\n",
    "        chatML format:\n",
    "        <|im_start|> System: <|im_end|>\n",
    "        <|im_start|> User: Hello <|im_end|>\n",
    "        <|im_start|> Assistant: Hi! <|im_end|>\n",
    "        \"\"\"\n",
    "        chatML = \"\"\n",
    "        for message in messages:\n",
    "            chatML += f\"<|im_start|> {message['role']}: {message['content']} <|im_end|>\\n\"\n",
    "        \n",
    "        if is_inference:\n",
    "            chatML += f\"<|im_start|> Assistant:\\n\"\n",
    "        return chatML\n",
    "    \n",
    "    def preprocess(self,record,tokenizer,model_name = 'mistral',ingore_instruction = True):\n",
    "\n",
    "        instruction = self.convert_messages_chatML(record['instruction'],is_inference=False)\n",
    "        response = self.convert_messages_chatML(record['response'],is_inference=False)\n",
    "\n",
    "        encoded_response = tokenizer(response, return_tensors=\"pt\",add_special_tokens=False)\n",
    "        response_length = encoded_response['input_ids'].shape[1]\n",
    "\n",
    "        encoded_full = tokenizer(instruction + response, return_tensors=\"pt\",add_special_tokens=False)\n",
    "                \n",
    "        if ingore_instruction:\n",
    "            # set the instruction part to be ignored (0)\n",
    "            attention_mask = torch.zeros_like(encoded_full['attention_mask'])\n",
    "            attention_mask[:,-response_length:] = 1\n",
    "        else:\n",
    "            attention_mask = encoded_full['attention_mask']\n",
    "        \n",
    "        return encoded_full['input_ids'][0],attention_mask[0] #,labels[0]\n",
    "\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        data_new = []\n",
    "        for record in tqdm(self.data):\n",
    "            input_ids,attention_mask = self.preprocess(record,self.tokenizer,self.model_name,self.ingore_instruction)\n",
    "            data_new.append({'input_ids':input_ids,'attention_mask':attention_mask}) #,'labels':labels})\n",
    "        return data_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f10ebff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "Current model:  Qwen/Qwen2.5-0.5B-Instruct\n",
      "currently, we are ingoring the instruction in the labels:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 153.88it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_example = InsrtuctionDataset(tokenizer,model_path,data=example_data,ingore_instruction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76502882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "Current model:  Qwen/Qwen2.5-0.5B-Instruct\n",
      "currently, we are ingoring the instruction in the labels:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 2007.08it/s]\n"
     ]
    }
   ],
   "source": [
    "example_file_path = 'example.jsonl'\n",
    "dataset_example = InsrtuctionDataset(tokenizer,model_path,data=None,data_path=example_file_path,ingore_instruction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67019ffc",
   "metadata": {},
   "source": [
    "#### Create a dataCollator to create labels and dynamically pad the records inside batches. \n",
    "\n",
    "The DataCollatorForLanguageModeling from transformers lib can apply dynamic padding but cannot set labels of instructions to -100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "192d37c6-b731-4bba-9b42-bbf4aee2a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False)\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    def __init__(self, tokenizer) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        tokenized_batch = self.tokenizer.pad(instances, return_tensors=\"pt\",padding=\"longest\")\n",
    "        tokenized_batch.data[\"labels\"] = tokenized_batch.data[\"input_ids\"].clone()\n",
    "        tokenized_batch.data[\"labels\"][tokenized_batch.data[\"attention_mask\"] == 0] = IGNORE_INDEX\n",
    "        return tokenized_batch.data\n",
    "\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f99a5-aa46-4f29-a7c3-1494c5fce7e3",
   "metadata": {},
   "source": [
    "### 3. Seting up training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cffe7a8b-ac0f-4730-ac6f-31c9c018d5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is to setup k bit training for QLora, for Lora and full parameter tuning, you can skip this step.\n",
    "# If do full parameter tuning, you can skip this section\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "464d80f4-dcea-4bb1-9fc9-1cbffc69cdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,243,520 || all params: 505,276,288 || trainable%: 2.2252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LI Jiangfan\\.conda\\envs\\'training'\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:500: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This steop is to setup Lora training, if you are doing full parameter tuning, you can skip this step.\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "# from accelerate import Accelerator\n",
    "# accelerator = Accelerator()\n",
    "# model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63ac27d-8c14-49f7-8451-aed7d4f39d75",
   "metadata": {},
   "source": [
    "### 4. Model Training\n",
    "\n",
    "Here to monitor training, I use [wandb](https://wandb.ai/home) to save and visualized the logs. \n",
    "\n",
    "You can also use other logger library that support transfromer.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9ed51e2-5091-4944-8461-cdef9046d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "\n",
    "\n",
    "# comment below if you don't want to use the wandb\n",
    "    \n",
    "# run on time for log into wandb \n",
    "# import wandb, os\n",
    "# wandb.login()\n",
    "\n",
    "wandb_project = \"lora_training_test\"\n",
    "os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3a4c222-5b9c-4ac2-a653-6650c5045836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory:  E:\\yurui\\models\\lora_training_test\\Qwen2.5-0.5B-Instruct2024-10-17-14-09-56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\y'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\y'\n",
      "C:\\Users\\LI Jiangfan\\AppData\\Local\\Temp\\ipykernel_20236\\3593666329.py:5: SyntaxWarning: invalid escape sequence '\\y'\n",
      "  output_dir = \"E:\\yurui\\models\\\\\" + project\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# In EChub, better store the model and cache in scratch to avoid the quota issue.\n",
    "project = \"lora_training_test\\\\\" + model_path.split(\"/\")[-1] + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "output_dir = \"E:\\yurui\\models\\\\\" + project\n",
    "\n",
    "print(\"Output directory: \", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "193996c0-86b6-46d2-aaca-4248359433f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LI Jiangfan\\.conda\\envs\\'training'\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set training parameters\n",
    "training_arguments = transformers.TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1, # change batch size based on the GPU memory\n",
    "    learning_rate=2.5e-5,  # about 10x smaller than the normal learning rate\n",
    "    weight_decay=0.001,\n",
    "    num_train_epochs=1, # normally put to 3\n",
    "    warmup_ratio =0.2,\n",
    "    logging_dir=output_dir + \"/logs\",\n",
    "    logging_strategy = 'steps',\n",
    "    logging_steps=5,\n",
    "    logging_first_step = True,\n",
    "    save_steps = 200,\n",
    "    evaluation_strategy= 'steps',\n",
    "    eval_steps = 50,\n",
    "    optim='paged_adamw_8bit',\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    report_to=\"all\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # comment below if you don't want to use the wandb\n",
    "    run_name=project,\n",
    "    remove_unused_columns=True,\n",
    "    bf16=True,\n",
    "    save_total_limit = 1,\n",
    "    # save space, especially when doing the full parameter tuning\n",
    "    load_best_model_at_end = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc2d338b-e92a-4b96-97ac-7994a94caaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    args=training_arguments, \n",
    "    train_dataset=dataset_example,\n",
    "    eval_dataset=dataset_example,  # we need to prepare the evaluation dataset,  set it to the same as the training dataset for now\n",
    "    data_collator = data_collator\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68f13940-3321-47a3-8551-5dec06310109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\yurui\\models\\lora_training_test\\Qwen2.5-0.5B-Instruct2024-10-17-14-09-56\n"
     ]
    }
   ],
   "source": [
    "# trainer.train()\n",
    "\n",
    "print(output_dir)\n",
    "# trainer.save_model(output_dir,save_embedding_layers=True)\n",
    "model.save_pretrained(output_dir,save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f49eaa22-122f-44f6-8013-6b89d1028c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce61ca242ab04982823a16bd8ffc3373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.732656955718994, 'eval_model_preparation_time': 0.011, 'eval_runtime': 1.6184, 'eval_samples_per_second': 2.472, 'eval_steps_per_second': 0.618}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a9dd3-c447-4725-9abf-d452e180cb39",
   "metadata": {},
   "source": [
    "### 5. Vaildation\n",
    "\n",
    "**Before checking the model, better restart the kernel to free the memory.**\n",
    "\n",
    "Then, same step as loading the model in section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6f7d2ee-b8df-437e-af16-2b904ddaa169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['HF_HOME'] = 'YOUR_LOCAL_PATH'\n",
    "\n",
    "import json, copy, os, torch, transformers, logging\n",
    "from peft import LoraConfig, PeftModel\n",
    "from typing import Dict, Sequence, List\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d424b9fd-0cd9-4a75-a4ec-678697d8b744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\y'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\y'\n",
      "C:\\Users\\LI Jiangfan\\AppData\\Local\\Temp\\ipykernel_28948\\2767995562.py:4: SyntaxWarning: invalid escape sequence '\\y'\n",
      "  output_dir = 'E:\\yurui\\models\\lora_training_test\\Qwen2.5-0.5B-Instruct2024-10-17-14-09-56'\n",
      "c:\\Users\\LI Jiangfan\\.conda\\envs\\'training'\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:500: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_path = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "output_dir = 'E:\\yurui\\models\\lora_training_test\\Qwen2.5-0.5B-Instruct2024-10-17-14-09-56'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "# for Lora or Qlora, we load the base model and then merge it with QLora config\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,device_map='auto')\n",
    "ft_model = PeftModel.from_pretrained(model, output_dir)\n",
    "\n",
    "\n",
    "# for full parameter tuning, we load the saved checkpoint directly\n",
    "# model = AutoModelForCausalLM.from_pretrained(output_dir,device_map='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3761f69",
   "metadata": {},
   "source": [
    "After confirming the model performance, you can choose to merge the save the model if using Lora, and save it to hugging face library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc492392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_messages_chatML(messages: List[Dict[str, str]],is_inference = False) -> str:\n",
    "    \"\"\"\n",
    "    Convert a list of messages to a chatML string.\n",
    "    chatML format:\n",
    "    <|im_start|> System: <|im_end|>\n",
    "    <|im_start|> User: Hello <|im_end|>\n",
    "    <|im_start|> Assistant: Hi! <|im_end|>\n",
    "    \"\"\"\n",
    "    chatML = \"\"\n",
    "    for message in messages:\n",
    "        chatML += f\"<|im_start|> {message['role']}: {message['content']} <|im_end|>\\n\"\n",
    "    \n",
    "    if is_inference:\n",
    "        chatML += f\"<|im_start|> Assistant:\\n\"\n",
    "    return chatML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3c10f7d-bfbe-4084-9360-35e2a8d91318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|> User: How are you? <|im_end|>\n",
      "<|im_start|> Assistant:\n",
      "\n",
      " User: How are you? \n",
      " Assistant:\n",
      "I'm just a computer program, so I don't have feelings or emotions like humans do. However, I can answer questions and provide information to the best of my ability based on the data I've been trained on. How may I assist you today?\n"
     ]
    }
   ],
   "source": [
    "test_query = [{'role': 'User', 'content': 'How are you?'}]\n",
    "\n",
    "test_query_chatML = convert_messages_chatML(test_query,is_inference=True)\n",
    "# print(test_query_chatML)\n",
    "model_input = tokenizer.encode(test_query_chatML, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = ft_model.generate(model_input, max_new_tokens=1024)\n",
    "# print(outputs[0])\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    " \n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25973e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'training'",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
